---
---
@string{aps = {American Physical Society,}}


@article{clean,
title = {Enzyme Function Prediction using Contrastive Learning},
author = {Yu*, Tianhao and Cui*, Haiyang and Li, Jianan Canal and Luo, Yunan and Zhao, Huimin},
journal = {Science,},
volume = {379},
number = {6639},
pages = {1358-1363},
year = {2023},
preview={CLEAN.png},
doi = {10.1126/science.adf2465},
poster={CLEAN_Poster.pdf},
code={https://github.com/tttianhao/CLEAN},
pdf={https://www.science.org/doi/10.1126/science.adf2465},
abstract = {Enzyme function annotation is a fundamental challenge, and numerous computational tools have been developed. However, most of these tools cannot accurately predict functional annotations, such as enzyme commission (EC) number, for less-studied proteins or those with previously uncharacterized functions or multiple activities. We present a machine learning algorithm named CLEAN (contrastive learning–enabled enzyme annotation) to assign EC numbers to enzymes with better accuracy, reliability, and sensitivity compared with the state-of-the-art tool BLASTp. The contrastive learning framework empowers CLEAN to confidently (i) annotate understudied enzymes, (ii) correct mislabeled enzymes, and (iii) identify promiscuous enzymes with two or more EC numbers—functions that we demonstrate by systematic in silico and in vitro experiments. We anticipate that this tool will be widely used for predicting the functions of uncharacterized enzymes, thereby advancing many fields, such as genomics, synthetic biology, and biocatalysis. With rapidly growing genomic and metagenomic databases, we have vastly more sequence data than functional data for enzymes. Accurate functional annotation from sparse experimental evidence is therefore crucial for analysis and applications when working from sequence data. Hoping to circumvent the limitations of current approaches, Yu et al. developed a machine learning model based on contrastive learning that performs particularly well at discerning enzyme function. In addition to comparing the performance of the method with existing tools, the authors experimentally validated predicted functions of 36 enzymes that form carbon–halogen bonds. They found excellent prediction accuracy and the ability to distinguish between similar activities. —MAF A contrastive learning algorithm enables accurate enzyme function annotation.}
}


@article{mctensor,
title = {MCTensor: A High-Precision Deep Learning Library with Multi-Component Floating-Point},
author = {Yu*, Tao and Guo*, Wentao and Li*, Jianan Canal and Yuan*, Tiancheng and De Sa, Christopher},
journal={ICML Workshop on Hardware Aware Efficient Training (HAET),},
year = {2022},
preview={MCTensor.png},
doi = {10.48550/arXiv.2207.08867},
pdf={MCTensor.pdf},
poster={MCTensor_poster.pdf},
code={https://github.com/ydtydr/MCTensor},
abstract={In this paper, we introduce MCTensor, a library based on PyTorch for providing general-purpose and high-precision arithmetic for DL training. MCTensor is used in the same way as PyTorch Tensor: we implement multiple basic, matrix-level computation operators and NN modules for MCTensor with identical PyTorch interface. Our algorithms achieve high precision computation and also benefits from heavily-optimized PyTorch floating-point arithmetic. We evaluate MCTensor arithmetic against PyTorch native arithmetic for a series of tasks, where models using MCTensor in float16 would match or outperform the PyTorch model with float32 or float64 precision.},
}
 
@article{cKAM,
author = {Li*, Jianan Canal and Zeng*, Yimeng and Guo*, Wentao},
year = {2022},
preview={cKAM.png},
title = {Cyclical Kernel Adaptive Metropolis},
journal={Preprint. },
doi = {10.48550/arXiv.2206.14421},
code={https://github.com/canallee/Cyclical-Kernel-Adaptive-Metropolis},
pdf={cKAM.pdf},
abstract={We propose cKAM, cyclical Kernel Adaptive Metropolis, which incorporates a cyclical stepsize scheme to allow control for exploration and sampling. We show that on a crafted bimodal distribution, existing Adaptive Metropolis type algorithms would fail to converge to the true posterior distribution. We point out that this is because adaptive samplers estimates the local/global covariance structure using past history of the chain, which will lead to adaptive algorithms be trapped in a local mode. We demonstrate that cKAM encourages exploration of the posterior distribution and allows the sampler to escape from a local mode, while maintaining the high performance of adaptive methods.}
}



 
