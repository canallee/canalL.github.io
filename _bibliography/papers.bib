---
---
@string{aps = {American Physical Society,}}

@article{clean,
title = {PseudoMSA: Towards High-fitness Protein Variant Generation Guided by Protein Language Models},
author = { Li, Jianan Canal and Yu, Tianhao and Luo, Yunan and Zhao, Huimin},
journal={In preparation,},
year = {2022},
preview={pMSA.png},
code={https://github.com/canallee/PseudoMSA},
project={https://github.com/canallee/PseudoMSA#readme},
abstract={We present a novel generative pipeline that can efficiently recommend high-order protein variants with high fitness. Leveraging on language models trained on billions of naturally occurring proteins, our framework only uses wild-type sequences but not other prior information such as Multiple Sequence Alignment (MSAs) or mutagenesis data. Starting with only the wild-type protein sequence, our framework first generates “pseudo-MSA” by applying masking-filling tasks using a protein language model. Micing evolution, our framework iteratively masks positions based on previous sequences. To evaluate the plausibility of the introduction of new mutations, we calculate the likelihood of the mutated residue relative to the previous sequences, which models the degree of `intrinsic fitness' gain. After the pseudoMSA is generated, it is used as the training data of a generative model tasked to generate high-order variants. The trained generative model can be used as a high-fidelity prior to sample high-fitness, high-order protein variants. Our presented work can be widely applied to downstream protein engineering tasks, and can potentially recover higher-order mutants which can be easily neglected in the current bioinformatics workflow. }
}



@article{clean,
title = {Enzyme Function Prediction using Contrastive Learning},
author = {Yu*, Tianhao and Cui*, Haiyang and Li, Jianan Canal and Luo, Yunan and Zhao, Huimin},
journal={In revision at Science,},
year = {2022},
preview={CLEAN.png},
poster={CLEAN_Poster.pdf},
notes={CLEAN_proofs.pdf},
code={https://github.com/tttianhao/CLEAN},
abstract={Enzyme function annotation is a fundamental challenge in protein science and numerous
computational tools have been developed. However, most of these tools cannot accurately predict functional
annotations such as enzyme commission (EC) number for less studied or proteins with novel or multiple
functions. Herein, we present a machine learning algorithm named CLEAN (contrastive learning enabled
enzyme annotation) to assign EC numbers to enzymes with better accuracy, reliability, and sensitivity than
the state-of-the-art tool BLASTp. CLEAN learns an enzyme function informed sequence representation
whose Euclidean distance reflects functional similarity. The transformative contrastive learning framework
empowers CLEAN to confidently: i) characterize the understudied or unannotated enzymes, ii) correct the
mislabeled enzymes, and iii) identify the promiscuous enzymes, which are demonstrated by systematic in
silico and in vitro experiments. We expect this tool to greatly facilitate functional genomics, enzymology,
and synthetic biology studies.}
}


@article{mctensor,
title = {MCTensor: A High-Precision Deep Learning Library with Multi-Component Floating-Point},
author = {Yu*, Tao and Guo*, Wentao and Li*, Jianan Canal and Yuan*, Tiancheng and De Sa, Christopher},
journal={ICML Workshop on Hardware Aware Efficient Training (HAET),},
year = {2022},
preview={MCTensor.png},
doi = {10.48550/arXiv.2207.08867},
pdf={MCTensor.pdf},
poster={MCTensor_poster.pdf},
slides={MCTensor_slides.pdf},
code={https://github.com/ydtydr/MCTensor},
abstract={In this paper, we introduce MCTensor, a library based on PyTorch for providing general-purpose and high-precision arithmetic for DL training. MCTensor is used in the same way as PyTorch Tensor: we implement multiple basic, matrix-level computation operators and NN modules for MCTensor with identical PyTorch interface. Our algorithms achieve high precision computation and also benefits from heavily-optimized PyTorch floating-point arithmetic. We evaluate MCTensor arithmetic against PyTorch native arithmetic for a series of tasks, where models using MCTensor in float16 would match or outperform the PyTorch model with float32 or float64 precision.},
}
 
@article{cKAM,
author = {Li*, Jianan Canal and Zeng*, Yimeng and Guo*, Wentao},
year = {2021},
preview={cKAM.png},
title = {Cyclical Kernel Adaptive Metropolis},
journal={Preprint. In submission,},
doi = {10.48550/arXiv.2206.14421},
code={https://github.com/canallee/Cyclical-Kernel-Adaptive-Metropolis},
pdf={cKAM.pdf},
abstract={We propose cKAM, cyclical Kernel Adaptive Metropolis, which incorporates a cyclical stepsize scheme to allow control for exploration and sampling. We show that on a crafted bimodal distribution, existing Adaptive Metropolis type algorithms would fail to converge to the true posterior distribution. We point out that this is because adaptive samplers estimates the local/global covariance structure using past history of the chain, which will lead to adaptive algorithms be trapped in a local mode. We demonstrate that cKAM encourages exploration of the posterior distribution and allows the sampler to escape from a local mode, while maintaining the high performance of adaptive methods.}
}



 